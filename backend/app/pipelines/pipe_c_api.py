"""
backend/app/pipelines/pipe_c_api.py
Pipeline C: é€šç”¨å¤§æ¨¡å‹ API è°ƒç”¨ (Via LiteLLM)
æ”¯æŒæ–‡æœ¬å¯¹è¯å’Œå›¾ç‰‡ç”Ÿæˆã€‚
"""
import logging
import base64
import httpx
import io
import os
from typing import Dict, Any
from app.websocket_manager import manager
from app.schemas import WSMessage
from app.utils import storage

logger = logging.getLogger("backend.pipe_c")

try:
    import litellm
    litellm.suppress_instrumentation = True # ç¦æ­¢å‘é€é¥æµ‹æ•°æ®
    logger.info("âœ… LiteLLM module loaded successfully.")
except ImportError as e:
    logger.error(f"âŒ LiteLLM import failed: {e}")
    litellm = None

# [New] Google GenAI SDK Support
try:
    from google import genai
    from google.genai import types
    from PIL import Image
    GOOGLE_GENAI_AVAILABLE = True
except ImportError:
    GOOGLE_GENAI_AVAILABLE = False

async def _load_image_pil(image_input: Any) -> Any:
    """Helper: Load image input as PIL Image for Google SDK"""
    try:
        data = None
        if isinstance(image_input, str):
            if image_input.startswith("http"):
                async with httpx.AsyncClient() as client:
                    resp = await client.get(image_input)
                    if resp.status_code == 200:
                        data = resp.content
            elif image_input.startswith("data:"):
                if "," in image_input:
                    _, encoded = image_input.split(",", 1)
                else:
                    encoded = image_input
                data = base64.b64decode(encoded)
            else:
                try:
                    data = base64.b64decode(image_input)
                except:
                    pass
        elif isinstance(image_input, bytes):
            data = image_input
            
        if data:
            return Image.open(io.BytesIO(data))
    except Exception as e:
        logger.warning(f"Failed to load PIL image: {e}")
    return None

async def _run_google_genai(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Native Google GenAI SDK call for Gemini models
    Supports: Chat, Vision, Image Generation (Imagen 3)
    """
    try:
        api_key = payload.get("api_key") or os.getenv("GEMINI_API_KEY")
        if not api_key:
             return {"status": "error", "message": "Missing API Key for Gemini"}

        client = genai.Client(api_key=api_key)
        
        # Clean model name (remove 'gemini/' prefix if coming from frontend)
        model = payload.get("model", "gemini-1.5-flash")
        if model.startswith("gemini/"):
            model = model.replace("gemini/", "", 1)

        contents = []
        
        # 1. Handle Chat History (messages)
        if payload.get("messages"):
            for msg in payload["messages"]:
                role = "user" if msg["role"] == "user" else "model"
                parts = []
                content = msg["content"]
                
                if isinstance(content, str):
                    parts.append(types.Part(text=content))
                elif isinstance(content, list):
                    for item in content:
                        if item["type"] == "text":
                            parts.append(types.Part(text=str(item["text"])))
                        elif item["type"] == "image_url":
                            img = await _load_image_pil(item["image_url"]["url"])
                            if img:
                                b = io.BytesIO()
                                fmt = img.format or "PNG"
                                img.save(b, format=fmt)
                                mime_type = f"image/{fmt.lower()}"
                                parts.append(types.Part(inline_data=types.Blob(mime_type=mime_type, data=b.getvalue())))
                
                if parts:
                    contents.append(types.Content(role=role, parts=parts))

        # 2. Handle Direct Prompt/Image (e.g. from Image Gen UI)
        elif payload.get("prompt"):
            parts = [types.Part(text=str(payload["prompt"]))]
            if payload.get("image"):
                img = await _load_image_pil(payload["image"])
                if img:
                    b = io.BytesIO()
                    fmt = img.format or "PNG"
                    img.save(b, format=fmt)
                    mime_type = f"image/{fmt.lower()}"
                    parts.append(types.Part(inline_data=types.Blob(mime_type=mime_type, data=b.getvalue())))
            contents.append(types.Content(role="user", parts=parts))

        # Config: Enable Image Generation
        config = types.GenerateContentConfig(
            response_modalities=["TEXT", "IMAGE"]
        )

        logger.info(f"ğŸš€ Google GenAI Call: {model}")

        # Execute in thread pool (SDK is sync)
        def _call_sync():
            return client.models.generate_content(
                model=model,
                contents=contents,
                config=config
            )
        
        import asyncio
        loop = asyncio.get_running_loop()
        response = await loop.run_in_executor(None, _call_sync)

        # Parse Response
        text_content = ""
        images = []
        
        if response.parts:
            for part in response.parts:
                if part.text:
                    text_content += part.text
                if part.inline_data:
                    # Convert raw bytes to base64 data URI
                    img_bytes = part.inline_data.data
                    
                    # Save to project if available
                    project_id = payload.get("project_id")
                    if project_id:
                        save_res = storage.save_generated_image(img_bytes, prefix="gemini_gen", project_id=project_id)
                        images.append(save_res["url"])
                    else:
                        b64_str = base64.b64encode(img_bytes).decode('utf-8')
                        mime = part.inline_data.mime_type or "image/png"
                        images.append(f"data:{mime};base64,{b64_str}")

        result_data = {
            "content": text_content,
            "images": images,
            "raw": str(response)
        }
        
        if not text_content and not images:
             result_data["content"] = "âš ï¸ Empty response from Gemini."

        return {"status": "success", "data": result_data}

    except Exception as e:
        logger.error(f"âŒ Google GenAI Error: {e}")
        return {"status": "error", "message": str(e)}

async def run(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    é€šç”¨å¤§æ¨¡å‹è°ƒç”¨æ¥å£ã€‚

    æ”¯æŒ "protocol": "litellm" æ¨¡å¼
    ç›´æ¥ä½¿ç”¨ Python litellm åº“è°ƒç”¨æ‰€æœ‰ä¸»æµå¤§æ¨¡å‹ã€‚
    Payload å‚æ•°:
    - protocol: "litellm"
    - model: æ¨¡å‹åç§° (å¦‚ "gpt-4", "claude-3-opus", "ollama/llama3")
    - messages: å¯¹è¯å†å²
    - api_key, base_url ç­‰å¯é€‰å‚æ•°

    æ”¯æŒ "protocol": "litellm_image" æ¨¡å¼
    è°ƒç”¨ DALL-E 3, Imagen ç­‰ç”Ÿå›¾æ¨¡å‹ã€‚
    Payload å‚æ•°:
    - protocol: "litellm_image"
    - model: "dall-e-3", "vertex_ai/imagen-3"
    - prompt: "A cute cat"
    """
    # --- 0. Google GenAI åŸç”Ÿè°ƒç”¨ (é’ˆå¯¹ Gemini æ¨¡å‹) ---
    model = payload.get("model", "").lower()
    if model.startswith("gemini") or "gemini" in model:
        if not GOOGLE_GENAI_AVAILABLE:
             return {"status": "error", "message": "google-genai library not installed. Please run `pip install google-genai`"}
        return await _run_google_genai(payload)

    # --- 1. LiteLLM åº“è°ƒç”¨æ¨¡å¼ ---
    if payload.get("protocol") == "litellm":
        if litellm is None:
            return {"status": "error", "message": "LiteLLM library not installed. Please run `pip install litellm`"}
        return await _run_litellm(payload)

    # --- 1.5 LiteLLM å›¾ç‰‡ç”Ÿæˆæ¨¡å¼ ---
    if payload.get("protocol") == "litellm_image":
        if litellm is None:
            return {"status": "error", "message": "LiteLLM library not installed. Please run `pip install litellm`"}
        return await _run_litellm_image(payload)

    return {"status": "error", "message": "Unknown protocol. Please use 'litellm' or 'litellm_image'."}

async def _run_litellm(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LiteLLM åº“è¿›è¡Œé€šç”¨å¤§æ¨¡å‹è°ƒç”¨
    """
    try:
        model = payload.get("model", "gpt-3.5-turbo")
        base_url = payload.get("base_url")
        api_key = payload.get("api_key")

        # [Fix] å¦‚æœæä¾›äº† base_url ä¸”æ¨¡å‹åæ²¡æœ‰æŒ‡å®šæä¾›å•†ï¼ˆä¸å« /ï¼‰ï¼Œåˆ™é»˜è®¤ä¸º openai/ åè®®
        # è¿™è§£å†³äº†ä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼ˆå¦‚ vLLM/Ollamaï¼‰æ—¶ litellm æŠ¥é”™ "LLM Provider NOT provided" çš„é—®é¢˜
        if base_url and "/" not in model:
            model = f"openai/{model}"

        # æ„é€ å‚æ•°
        kwargs = {
            "model": model,
            "messages": payload.get("messages", []),
            "stream": False,
        }

        # å¯é€‰å‚æ•°æ˜ å°„
        if api_key:
            kwargs["api_key"] = api_key
        elif base_url:
            # [Fix] æœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦ Keyï¼Œä½† OpenAI å®¢æˆ·ç«¯åº“å¯èƒ½è¦æ±‚éç©ºã€‚
            kwargs["api_key"] = "sk-dummy-key"

        if base_url:
            kwargs["api_base"] = base_url
            logger.info(f"ğŸ”— Using Custom Base URL: {base_url}")
            
            # [Fix] é’ˆå¯¹æœ¬åœ°æœåŠ¡ï¼Œå¼ºåˆ¶ç»•è¿‡ç³»ç»Ÿä»£ç†ï¼Œé˜²æ­¢ VPN æ‹¦æˆª localhost è¯·æ±‚
            if "localhost" in base_url or "127.0.0.1" in base_url:
                no_proxy = os.environ.get("NO_PROXY", "")
                if "localhost" not in no_proxy:
                    os.environ["NO_PROXY"] = f"{no_proxy},localhost,127.0.0.1".lstrip(",")
            
        # é€ä¼ å¸¸è§å‚æ•°
        for key in ["temperature", "max_tokens", "top_p", "stop", "frequency_penalty", "presence_penalty"]:
            if key in payload:
                kwargs[key] = payload[key]

        logger.info(f"ğŸš€ LiteLLM Call: {kwargs['model']}")
        
        # å¼‚æ­¥è°ƒç”¨ (acompletion æ˜¯ litellm çš„å¼‚æ­¥æ–¹æ³•)
        # æ³¨æ„ï¼šå¼€å¯ stream=True åï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ª AsyncGenerator
        response = await litellm.acompletion(**kwargs)
        
        # æå–æ–‡æœ¬å†…å®¹
        content = response.choices[0].message.content
        # è¿”å›æ ‡å‡†åŒ–ç»“æœ
        return {"status": "success", "data": {"content": content, "raw": response.model_dump()}}

    except Exception as e:
        logger.error(f"âŒ LiteLLM Error: {e}")
        return {"status": "error", "message": str(e)}

async def _run_litellm_image(payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä½¿ç”¨ LiteLLM åº“è¿›è¡Œå›¾ç‰‡ç”Ÿæˆ (DALL-E 3, Imagen 3 ç­‰)
    """
    try:
        model = payload.get("model", "dall-e-3")
        base_url = payload.get("base_url")
        api_key = payload.get("api_key")

        # [Fix] å¦‚æœæä¾›äº† base_url ä¸”æ¨¡å‹åæ²¡æœ‰æŒ‡å®šæä¾›å•†ï¼ˆä¸å« /ï¼‰ï¼Œåˆ™é»˜è®¤ä¸º openai/ åè®®
        if base_url and "/" not in model:
            model = f"openai/{model}"

        # æ„é€ å‚æ•°
        kwargs = {
            "model": model,
            "prompt": payload.get("prompt", ""),
        }
        
        if not kwargs["prompt"]:
             return {"status": "error", "message": "Missing 'prompt' in payload"}

        # å¯é€‰å‚æ•°æ˜ å°„
        if api_key:
            kwargs["api_key"] = api_key
        elif base_url:
            # [Fix] æœ¬åœ°æ¨¡å‹é€šå¸¸ä¸éœ€è¦ Keyï¼Œä½† OpenAI å®¢æˆ·ç«¯åº“å¯èƒ½è¦æ±‚éç©ºã€‚
            kwargs["api_key"] = "sk-dummy-key"

        if base_url:
            kwargs["api_base"] = base_url
            logger.info(f"ğŸ”— Using Custom Base URL: {base_url}")
            
            # [Fix] é’ˆå¯¹æœ¬åœ°æœåŠ¡ï¼Œå¼ºåˆ¶ç»•è¿‡ç³»ç»Ÿä»£ç†
            if "localhost" in base_url or "127.0.0.1" in base_url:
                no_proxy = os.environ.get("NO_PROXY", "")
                if "localhost" not in no_proxy:
                    os.environ["NO_PROXY"] = f"{no_proxy},localhost,127.0.0.1".lstrip(",")
            
        # [Fix] æ”¯æŒå›¾ç”Ÿå›¾/ç¼–è¾‘æ¨¡å¼ä¼ å…¥ image
        if payload.get("image"):
            img_input = payload["image"]
            # å¦‚æœæ˜¯ Data URLï¼Œè½¬æ¢ä¸º BytesIO å¯¹è±¡ (æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ )
            if isinstance(img_input, str) and img_input.startswith("data:"):
                try:
                    if "," in img_input:
                        _, encoded = img_input.split(",", 1)
                    else:
                        encoded = img_input
                    img_bytes = base64.b64decode(encoded)
                    img_file = io.BytesIO(img_bytes)
                    img_file.name = "image.png" # å¿…é¡»è®¾ç½®æ–‡ä»¶åï¼Œå¦åˆ™éƒ¨åˆ†åº“ä¼šæŠ¥é”™
                    kwargs["image"] = img_file
                except Exception as e:
                    logger.warning(f"Failed to decode base64 image: {e}")
                    kwargs["image"] = img_input
            else:
                kwargs["image"] = img_input

        # [New] æ”¯æŒ Mask (ç”¨äº Inpainting)
        if payload.get("mask"):
            mask_input = payload["mask"]
            if isinstance(mask_input, str) and mask_input.startswith("data:"):
                try:
                    if "," in mask_input:
                        _, encoded = mask_input.split(",", 1)
                    else:
                        encoded = mask_input
                    mask_bytes = base64.b64decode(encoded)
                    mask_file = io.BytesIO(mask_bytes)
                    mask_file.name = "mask.png"
                    kwargs["mask"] = mask_file
                except Exception:
                    kwargs["mask"] = mask_input
            else:
                kwargs["mask"] = mask_input
            
        # é€ä¼ å¸¸è§å‚æ•° (n=æ•°é‡, size=å°ºå¯¸, response_format=url/b64_json)
        for key in ["n", "size", "response_format", "quality", "style"]:
            if key in payload:
                kwargs[key] = payload[key]

        logger.info(f"ğŸš€ LiteLLM Image Gen: {kwargs['model']}")
        
        # å¼‚æ­¥è°ƒç”¨
        response = await litellm.aimage_generation(**kwargs)
        logger.info(f"ğŸ“¸ Raw Response: {response}")
        
        # æå–ç»“æœ (å…¼å®¹ OpenAI æ ¼å¼å¯¹è±¡æˆ–å­—å…¸)
        images = []
        data_items = []
        project_id = payload.get("project_id")

        if hasattr(response, 'data'):
            data_items = response.data
        elif isinstance(response, dict) and 'data' in response:
            data_items = response['data']

        for item in data_items:
            val = None
            # å°è¯•å¯¹è±¡å±æ€§è®¿é—® (å¿½ç•¥ AttributeError)
            try:
                val = getattr(item, 'url', None) or getattr(item, 'b64_json', None)
            except AttributeError:
                pass
            
            # å°è¯•å­—å…¸è®¿é—®
            if not val and isinstance(item, dict):
                val = item.get('url') or item.get('b64_json')
            
            if val:
                # [Fix] å¦‚æœæ˜¯ Base64 ä¸”æ²¡æœ‰å‰ç¼€ï¼Œè¡¥ä¸Šå‰ç¼€
                if isinstance(val, str) and len(val) > 200 and not val.startswith('http') and not val.startswith('data:'):
                    val = f"data:image/png;base64,{val}"
                
                # [New] è‡ªåŠ¨ä¿å­˜åˆ°é¡¹ç›®æ–‡ä»¶å¤¹
                if project_id:
                    try:
                        image_content = None
                        # æƒ…å†µ A: è¿œç¨‹ URL (å¦‚ DALL-E 3) -> ä¸‹è½½å¹¶ä¿å­˜
                        if val.startswith("http"):
                            async with httpx.AsyncClient() as client:
                                resp = await client.get(val)
                                if resp.status_code == 200:
                                    image_content = resp.content
                        
                        # æƒ…å†µ B: Base64 (å¦‚ Stable Diffusion/Gemini) -> è§£ç å¹¶ä¿å­˜
                        elif val.startswith("data:image"):
                            try:
                                _, encoded = val.split(",", 1)
                                image_content = base64.b64decode(encoded)
                            except Exception:
                                pass
                        
                        if image_content:
                            save_result = storage.save_generated_image(image_content, prefix="ai_gen", project_id=project_id)
                            val = save_result["url"] # æ›¿æ¢ä¸ºæœ¬åœ° URL
                            logger.info(f"ğŸ’¾ Saved AI image to: {val}")
                    except Exception as e:
                        logger.error(f"Failed to save generated image: {e}")

                images.append(val)
        
        # è¿”å›æ ‡å‡†åŒ–ç»“æœ
        result_data = {
            "images": images, 
            "raw": response.model_dump() if hasattr(response, "model_dump") else str(response)
        }

        # [Fix] å¦‚æœæ²¡æœ‰ç”Ÿæˆå›¾ç‰‡ï¼Œè¿”å›æç¤ºä¿¡æ¯é˜²æ­¢å‰ç«¯å¡æ­»
        if not images:
            result_data["content"] = "âš ï¸ ç”Ÿæˆç»“æœä¸ºç©º (No images returned)ã€‚\nè¯·æ£€æŸ¥ä¸‹æ–¹åŸå§‹å“åº”ä»¥æ’æŸ¥é—®é¢˜:\n" + str(result_data["raw"])

        return {
            "status": "success", 
            "data": result_data
        }

    except Exception as e:
        logger.error(f"âŒ LiteLLM Image Error: {e}")
        return {"status": "error", "message": str(e)}
